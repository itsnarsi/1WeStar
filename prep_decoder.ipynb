{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(29)\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "cudnn.benchmark = True\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from glob import glob\n",
    "from PIL.PngImagePlugin import PngImageFile, PngInfo\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class quantclip(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(self, input, quant):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        self.save_for_backward(input)\n",
    "        c = (input.clamp(min=-1, max =1)+1)/2.0 * quant\n",
    "        c = 2 * (c.round()/quant) - 1\n",
    "        return c\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < -1] = 0\n",
    "        grad_input[input > 1] = 0\n",
    "        return grad_input, None\n",
    "\n",
    "class QuantCLIP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_bits, dtype = torch.cuda.FloatTensor):\n",
    "        super(QuantCLIP, self).__init__()\n",
    "\n",
    "        self.quant = 2 ** num_bits - 1\n",
    "        self.quantclip = quantclip\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.quantclip.apply(input, self.quant)\n",
    "\n",
    "def getHAARFilters(num_filters):\n",
    "    LL = np.asarray([[0.5, 0.5], [0.5, 0.5]])\n",
    "    LH = np.asarray([[-0.5, -0.5], [0.5, 0.5]])\n",
    "    HL = np.asarray([[-0.5, 0.5], [-0.5, 0.5]])\n",
    "    HH = np.asarray([[0.5, -0.5], [-0.5, 0.5]])\n",
    "\n",
    "    DWT = np.concatenate((LL[np.newaxis, ...],\n",
    "                          LH[np.newaxis, ...],\n",
    "                          HL[np.newaxis, ...],\n",
    "                          HH[np.newaxis, ...]))[:, np.newaxis, ...]\n",
    "    DWT = np.float32(DWT)\n",
    "    DWT = torch.from_numpy(DWT)\n",
    "\n",
    "    return DWT.repeat(num_filters, 1, 1, 1)\n",
    "\n",
    "class HaarDWT(torch.nn.Module):\n",
    "    def __init__(self, in_ch = 1):\n",
    "        super(HaarDWT, self).__init__()\n",
    "\n",
    "        weights = getHAARFilters(in_ch)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_ch, in_ch * 4, 2, stride=2, bias=False, groups = in_ch)\n",
    "        self.conv.weight.data = weights\n",
    "        self.conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class HaarIDWT(torch.nn.Module):\n",
    "    def __init__(self, out_ch = 1):\n",
    "        super(HaarIDWT, self).__init__()\n",
    "\n",
    "        weights = getHAARFilters(out_ch)\n",
    "\n",
    "        self.conv = nn.ConvTranspose2d(out_ch * 4, out_ch, 2, stride=2, bias=False, groups = out_ch)\n",
    "        self.conv.weight.data = weights\n",
    "        self.conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Single CONV blocks:\n",
    "\"\"\"\n",
    "class BLOCK_3x3(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_ch, out_ch, ker, stride = 1\n",
    "        ):\n",
    "        super(BLOCK_3x3, self).__init__()\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.ReflectionPad2d(ker//2),\n",
    "            nn.Conv2d(in_ch, out_ch, ker, stride = stride, bias = True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feat(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Residual CONV blocks:\n",
    "\"\"\"\n",
    "class RES_3x3_BLOCK1(nn.Module):\n",
    "    \"\"\"\n",
    "        Residual Block:\n",
    "            [INPUT] -> 2*[CONV 3x3] -> [OUTPUT] + [INPUT]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, in_ch, out_ch, ker, squeeze = 2, res_scale = 0.25\n",
    "        ):\n",
    "        super(RES_3x3_BLOCK1, self).__init__()\n",
    "\n",
    "        self.skip = in_ch == out_ch\n",
    "        self.rs = res_scale\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BLOCK_3x3(in_ch, out_ch//squeeze, ker),\n",
    "            nn.BatchNorm2d(out_ch//squeeze),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BLOCK_3x3(out_ch//squeeze, out_ch, ker),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feat(x)\n",
    "        if self.skip: out = self.rs * out + x\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "Enocder:\n",
    "\"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.E = nn.Sequential(\n",
    "            HaarDWT(3),HaarDWT(12),\n",
    "            BLOCK_3x3(in_ch = 48, out_ch = 96, ker = 3, stride = 1),\n",
    "            RES_3x3_BLOCK1(in_ch = 96, out_ch = 96, ker = 3, squeeze = 2, res_scale = 1.0),\n",
    "            RES_3x3_BLOCK1(in_ch = 96, out_ch = 96, ker = 3, squeeze = 2, res_scale = 1.0),\n",
    "            nn.Conv2d(96, 3, 1),\n",
    "            QuantCLIP(8)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=0.25, mode='bilinear')\n",
    "        x = self.E(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "Deocder:\n",
    "\"\"\"\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.D = nn.Sequential(\n",
    "            BLOCK_3x3(in_ch = 3, out_ch = 96, ker = 3, stride = 1),\n",
    "            RES_3x3_BLOCK1(in_ch = 96, out_ch = 96, ker = 3, squeeze = 2, res_scale = 1.0),\n",
    "            RES_3x3_BLOCK1(in_ch = 96, out_ch = 96, ker = 3, squeeze = 2, res_scale = 1.0),\n",
    "            RES_3x3_BLOCK1(in_ch = 96, out_ch = 96, ker = 3, squeeze = 2, res_scale = 1.0),\n",
    "            nn.Conv2d(96, 48, 1),\n",
    "            HaarIDWT(12),HaarIDWT(3),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        self.U = nn.Sequential(\n",
    "            BLOCK_3x3(in_ch = 3, out_ch = 64, ker = 3, stride = 1),\n",
    "            RES_3x3_BLOCK1(in_ch = 64, out_ch = 64, ker = 3, squeeze = 2, res_scale = 1.0),\n",
    "            RES_3x3_BLOCK1(in_ch = 64, out_ch = 64, ker = 3, squeeze = 2, res_scale = 1.0),\n",
    "            nn.Conv2d(64, 12, 1),\n",
    "            HaarIDWT(3),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.D(x)\n",
    "        x = self.U(x)\n",
    "        x = self.U(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "de_model = Decoder()\n",
    "check_point_file = \"/home/cibitaw1/local/1WeStar/weights/submission_weights/decode.pth\"\n",
    "checkpoint = torch.load(check_point_file)\n",
    "de_model.load_state_dict(checkpoint, strict = False)\n",
    "de_model.cuda()\n",
    "print('.')\n",
    "\n",
    "en_model = Encoder()\n",
    "check_point_file = \"/home/cibitaw1/local/1WeStar/weights/submission_weights/encode.pth\"\n",
    "checkpoint = torch.load(check_point_file)\n",
    "en_model.load_state_dict(checkpoint, strict = False)\n",
    "en_model.cuda()\n",
    "print('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de_model = Decoder()\n",
    "check_point_file = \"/media/cibitaw1/DATA/SP2020/compressACT/weights/\"+\\\n",
    "\"QuantACTShuffleV8_exp01/checkpoint.pth.tar\"\n",
    "checkpoint = torch.load(check_point_file)\n",
    "de_model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "de_model.cuda()\n",
    "print('.')\n",
    "\n",
    "en_model = Encoder()\n",
    "check_point_file = \"/media/cibitaw1/DATA/SP2020/compressACT/weights/\"+\\\n",
    "\"QuantACTShuffleV8_exp01/checkpoint.pth.tar\"\n",
    "checkpoint = torch.load(check_point_file)\n",
    "en_model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "en_model.cuda()\n",
    "print('.')\n",
    "torch.save(de_model.state_dict(), \"/home/cibitaw1/local/1WeStar/weights/submission_weights/decode.pth\")\n",
    "torch.save(en_model.state_dict(), \"/home/cibitaw1/local/1WeStar/weights/submission_weights/encode.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compress(I_org, model):\n",
    "\n",
    "    e_ = 512\n",
    "    c_ = 16\n",
    "    d_ = e_ // c_\n",
    "    pad_ = 4\n",
    "\n",
    "    w, h = I_org.size\n",
    "\n",
    "    comp_w_new = np.ceil(w/c_)\n",
    "    comp_h_new = np.ceil(h/c_)\n",
    "\n",
    "    new_w = int(e_ * np.ceil(w/e_))\n",
    "    new_h = int(e_ * np.ceil(h/e_))\n",
    "\n",
    "    com_w = new_w // c_\n",
    "    com_h = new_h // c_\n",
    "\n",
    "    I = np.uint8(I_org).copy()\n",
    "    I = np.pad(I, ((0, int(new_h - h)),\n",
    "                   (0, int(new_w - w)),\n",
    "                   (0, 0)), mode = \"reflect\")\n",
    "    I = Image.fromarray(I)\n",
    "\n",
    "\n",
    "    I1 = np.float32(I)/255.0\n",
    "    I1 = np.transpose(I1, [2, 0, 1])\n",
    "\n",
    "    Enout = np.zeros((3, com_w, com_h))\n",
    "    Enout_w = np.zeros((3, com_w, com_h))\n",
    "    for i in list(np.arange(0, new_w, e_)):\n",
    "        for j in list(np.arange(0, new_h, e_)):\n",
    "            if i == 0:\n",
    "                x1 = int(i)\n",
    "                x2 = int((i + e_) + (pad_*2*c_))\n",
    "            else:\n",
    "                x1 = int(i - (pad_*c_))\n",
    "                x2 = int((i + e_) + (pad_*c_))\n",
    "\n",
    "            if j == 0:\n",
    "                y1 = int(j)\n",
    "                y2 = int((j + e_) + (pad_*2*c_))\n",
    "            else:\n",
    "                y1 = int(j - (pad_*c_))\n",
    "                y2 = int((j + e_) + (pad_*c_))\n",
    "            It = torch.from_numpy(np.expand_dims(I1[:, x1:x2, y1:y2], 0))\n",
    "            Xe = model(It.cuda())\n",
    "            Xe = (Xe + 1.0)/2.0\n",
    "            print(Xe.size())\n",
    "            print([ x1//c_,x2//c_, y1//c_, y2//c_])\n",
    "            Enout[:, x1//c_:x2//c_, y1//c_:y2//c_] += Xe.data.squeeze().cpu().numpy()\n",
    "            Enout_w[:, x1//c_:x2//c_, y1//c_:y2//c_] += 1.0\n",
    "\n",
    "    Enout = Enout/Enout_w\n",
    "    Enout = np.uint8(255 * Enout.transpose([1, 2, 0]))\n",
    "\n",
    "    Enout = Image.fromarray(Enout).crop((0, 0, comp_w_new, comp_h_new))\n",
    "\n",
    "    return Enout\n",
    "\n",
    "def compress2(I_org, model):\n",
    "    \n",
    "    c_ = 16\n",
    "    \n",
    "    w, h = I_org.size\n",
    "    \n",
    "    comp_w_new = np.ceil(w/c_)\n",
    "    comp_h_new = np.ceil(h/c_)\n",
    "    \n",
    "    I = np.uint8(I_org).copy()\n",
    "    I = np.pad(I, ((0, int(c_*np.ceil(I.shape[0]/c_) - I.shape[0])), \n",
    "                   (0, int(c_*np.ceil(I.shape[1]/c_) - I.shape[1])), \n",
    "                   (0, 0)), mode = \"reflect\")\n",
    "    I = Image.fromarray(I)\n",
    "    \n",
    "    \n",
    "    I1 = np.float32(I)/255.0\n",
    "    I1 = np.transpose(I1, [2, 0, 1])\n",
    "    I1 = torch.from_numpy(np.expand_dims(I1, 0))\n",
    "    \n",
    "    Xe = model(I1.cuda())\n",
    "    Xe = (Xe + 1.0)/2.0\n",
    "    Enout = Xe.data.squeeze().cpu().numpy()\n",
    "    \n",
    "    Enout = np.uint8(255 * Enout.transpose([1, 2, 0]))\n",
    "    \n",
    "    Enout = Image.fromarray(Enout).crop((0, 0, comp_w_new, comp_h_new))\n",
    "    \n",
    "    return Enout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decompress(EnIn, model):\n",
    "\n",
    "    e_ = 512\n",
    "    c_ = 16\n",
    "    d_ = e_ // c_\n",
    "    pad_ = 4\n",
    "\n",
    "    w, h = int(EnIn.text['w']), int(EnIn.text['h'])\n",
    "\n",
    "    comp_w_new = np.ceil(w/c_)\n",
    "    comp_h_new = np.ceil(h/c_)\n",
    "\n",
    "    new_w = int(e_ * np.ceil(w/e_))\n",
    "    new_h = int(e_ * np.ceil(h/e_))\n",
    "\n",
    "    com_w = new_w // c_\n",
    "    com_h = new_h // c_\n",
    "\n",
    "\n",
    "    I = np.zeros((3,new_h,new_w), dtype = np.float32)\n",
    "    I_w = np.zeros((3,new_h,new_w), dtype = np.float32)\n",
    "\n",
    "    EnIn = np.uint8(EnIn).copy()\n",
    "    EnIn = np.pad(EnIn, ((0, int(new_h - EnIn.shape[0])),\n",
    "                         (0, int(new_w - EnIn.shape[1])),\n",
    "                         (0, 0)), mode = \"reflect\")\n",
    "\n",
    "\n",
    "    EnIn = np.float32(EnIn)/255.0\n",
    "    EnIn = np.transpose(EnIn, [2, 0, 1])\n",
    "    for i in list(np.arange(0, com_w, d_)):\n",
    "        for j in list(np.arange(0, com_h, d_)):\n",
    "\n",
    "            if i == 0:\n",
    "                x1 = int(i)\n",
    "                x2 = int((i + d_) + pad_*2)\n",
    "            else:\n",
    "                x1 = int(i - pad_)\n",
    "                x2 = int((i + d_) + pad_)\n",
    "\n",
    "            if j == 0:\n",
    "                y1 = int(j)\n",
    "                y2 = int((j + d_) + pad_*2)\n",
    "            else:\n",
    "                y1 = int(j - pad_)\n",
    "                y2 = int((j + d_) + pad_)\n",
    "\n",
    "            It = torch.from_numpy(np.expand_dims(EnIn[:, x1:x2, y1:y2], 0))\n",
    "            It = It * 2.0 - 1.0\n",
    "            Xe = model(It.cuda())\n",
    "            I[:, x1*c_:x2*c_, y1*c_:y2*c_] += np.clip(Xe.data.squeeze().cpu().numpy(), 0, 1)\n",
    "            I_w[:, x1*c_:x2*c_, y1*c_:y2*c_] += 1.0\n",
    "\n",
    "    I = I/I_w\n",
    "\n",
    "    I = np.uint8(255 * I.transpose([1, 2, 0]))\n",
    "    I = Image.fromarray(I).crop((0, 0, w, h))\n",
    "\n",
    "    return I\n",
    "\n",
    "\n",
    "def decompress2(EnIn, model):\n",
    "    \n",
    "    c_ = 16\n",
    "    \n",
    "    w, h = int(EnIn.text['w']), int(EnIn.text['h'])\n",
    "    \n",
    "    \n",
    "    I = np.zeros((3,int(c_*np.ceil(h/c_)),int(c_*np.ceil(w/c_))),\n",
    "                 dtype = np.float32)\n",
    "    \n",
    "    new_en_w = I.shape[2]//c_\n",
    "    new_en_h = I.shape[1]//c_\n",
    "    \n",
    "                 \n",
    "    EnIn = np.uint8(EnIn).copy()\n",
    "    EnIn = np.pad(EnIn, ((0, int(new_en_h - EnIn.shape[0])), \n",
    "                         (0, int(new_en_w - EnIn.shape[1])), \n",
    "                         (0, 0)), mode = \"reflect\")\n",
    "    \n",
    "    \n",
    "    EnIn = np.float32(EnIn)/255.0\n",
    "    EnIn = np.transpose(EnIn, [2, 0, 1])\n",
    "    EnIn = torch.from_numpy(np.expand_dims(EnIn, 0))\n",
    "    EnIn = EnIn * 2.0 - 1.0\n",
    "    I = model(EnIn.cuda())\n",
    "    I = np.clip(I.data.squeeze().cpu().numpy(), 0, 1)\n",
    "    \n",
    "    I = np.uint8(255 * I.transpose([1, 2, 0]))\n",
    "    \n",
    "    I = Image.fromarray(I).crop((0, 0, w, h))\n",
    "    \n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = \"/media/cibitaw1/DATA/super_rez/professional_valid/valid/alberto-montalesi-176097.png\"\n",
    "I = Image.open(img_file).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 40, 40])\n",
      "[0, 40, 0, 40]\n",
      "torch.Size([1, 3, 40, 40])\n",
      "[0, 40, 28, 68]\n",
      "torch.Size([1, 3, 40, 40])\n",
      "[0, 40, 60, 100]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,40,36) (3,40,40) (3,40,36) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-31e112d0c2cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEnout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-2687735d7cf3>\u001b[0m in \u001b[0;36mcompress\u001b[0;34m(I_org, model)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mEnout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mEnout_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,40,36) (3,40,40) (3,40,36) "
     ]
    }
   ],
   "source": [
    "Enout = compress(I, en_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = PngInfo()\n",
    "metadata.add_text(\"w\", str(I.size[0]))\n",
    "metadata.add_text(\"h\", str(I.size[1]))\n",
    "Enout.save(\"test_en.png\", pnginfo=metadata)\n",
    "Enout = Image.open(\"test_en.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iout = decompress2(Enout, de_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_fldr = \"/media/cibitaw1/DATA/super_rez/professional_valid/valid\"\n",
    "imgs = glob(src_fldr + os.sep + \"*.png\")\n",
    "src_fldr = \"/media/cibitaw1/DATA/super_rez/mobile_valid/valid\"\n",
    "imgs += glob(src_fldr + os.sep + \"*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_fldr = \"/media/cibitaw1/DATA/super_rez/comp_test/compressed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/102 [00:00<?, ?it/s]/home/cibitaw1/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "100%|██████████| 102/102 [00:15<00:00,  9.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for img in tqdm(imgs):\n",
    "    I = Image.open(img).convert(\"RGB\")\n",
    "    Enout = compress2(I, en_model)\n",
    "    metadata = PngInfo()\n",
    "    metadata.add_text(\"w\", str(I.size[0]))\n",
    "    metadata.add_text(\"h\", str(I.size[1]))\n",
    "    img_name = os.path.join(dst_fldr, img.split(os.sep)[-1])\n",
    "    Enout.save(img_name, pnginfo=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
