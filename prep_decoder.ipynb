{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(29)\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "cudnn.benchmark = True\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from glob import glob\n",
    "from PIL.PngImagePlugin import PngImageFile, PngInfo\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpg_enc(I, out_f, enc_cmd, w, h):\n",
    "#     out_f = out_f\n",
    "    I.save(\"test_en.png\")\n",
    "    os.system(enc_cmd + ' -m 9 -f 444 -q 29 test_en.png -o ' + '\"'+out_f + '\"')\n",
    "    if not os.path.exists(out_f): print(out_f)\n",
    "    os.setxattr(out_f, 'user.h', bytes(str(h), 'utf-8'))\n",
    "    os.setxattr(out_f, 'user.w', bytes(str(w), 'utf-8'))\n",
    "    os.remove(\"test_en.png\")\n",
    "    \n",
    "def bpg_dec(bpg_enc_file, dec_cmd):\n",
    "#     bpg_enc_file = bpg_enc_file.replace(\" \", \"\\ \")\n",
    "    os.system(dec_cmd + ' \"' + bpg_enc_file + '\" -o test_de.png')\n",
    "    h = int(os.getxattr(bpg_enc_file, 'user.h'))\n",
    "    w = int(os.getxattr(bpg_enc_file, 'user.w'))\n",
    "    I = Image.open(\"test_de.png\")\n",
    "    return I, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class quantclip(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(self, input, quant):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return a\n",
    "        Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "        backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        self.save_for_backward(input)\n",
    "        c = (input.clamp(min=-1, max =1)+1)/2.0 * quant\n",
    "        c = 2 * (c.round()/quant) - 1\n",
    "        return c\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < -1] = 0\n",
    "        grad_input[input > 1] = 0\n",
    "        return grad_input, None\n",
    "\n",
    "class QuantCLIP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_bits, dtype = torch.cuda.FloatTensor):\n",
    "        super(QuantCLIP, self).__init__()\n",
    "\n",
    "        self.quant = 2 ** num_bits - 1\n",
    "        self.quantclip = quantclip\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.quantclip.apply(input, self.quant)\n",
    "\n",
    "def getHAARFilters(num_filters):\n",
    "    LL = np.asarray([[0.5, 0.5], [0.5, 0.5]])\n",
    "    LH = np.asarray([[-0.5, -0.5], [0.5, 0.5]])\n",
    "    HL = np.asarray([[-0.5, 0.5], [-0.5, 0.5]])\n",
    "    HH = np.asarray([[0.5, -0.5], [-0.5, 0.5]])\n",
    "\n",
    "    DWT = np.concatenate((LL[np.newaxis, ...],\n",
    "                          LH[np.newaxis, ...],\n",
    "                          HL[np.newaxis, ...],\n",
    "                          HH[np.newaxis, ...]))[:, np.newaxis, ...]\n",
    "    DWT = np.float32(DWT)\n",
    "    DWT = torch.from_numpy(DWT)\n",
    "\n",
    "    return DWT.repeat(num_filters, 1, 1, 1)\n",
    "\n",
    "class HaarDWT(torch.nn.Module):\n",
    "    def __init__(self, in_ch = 1):\n",
    "        super(HaarDWT, self).__init__()\n",
    "\n",
    "        weights = getHAARFilters(in_ch)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_ch, in_ch * 4, 2, stride=2, bias=False, groups = in_ch)\n",
    "        self.conv.weight.data = weights\n",
    "        self.conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class HaarIDWT(torch.nn.Module):\n",
    "    def __init__(self, out_ch = 1):\n",
    "        super(HaarIDWT, self).__init__()\n",
    "\n",
    "        weights = getHAARFilters(out_ch)\n",
    "\n",
    "        self.conv = nn.ConvTranspose2d(out_ch * 4, out_ch, 2, stride=2, bias=False, groups = out_ch)\n",
    "        self.conv.weight.data = weights\n",
    "        self.conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Single CONV blocks:\n",
    "\"\"\"\n",
    "class BLOCK_3x3(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_ch, out_ch, ker, stride = 1\n",
    "        ):\n",
    "        super(BLOCK_3x3, self).__init__()\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.ReflectionPad2d(ker//2),\n",
    "            nn.Conv2d(in_ch, out_ch, ker, stride = stride, bias = True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feat(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Residual CONV blocks:\n",
    "\"\"\"\n",
    "class RES_3x3_BLOCK1(nn.Module):\n",
    "    \"\"\"\n",
    "        Residual Block:\n",
    "            [INPUT] -> 2*[CONV 3x3] -> [OUTPUT] + [INPUT]\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, in_ch, out_ch, ker, squeeze = 2, res_scale = 0.25\n",
    "        ):\n",
    "        super(RES_3x3_BLOCK1, self).__init__()\n",
    "\n",
    "        self.skip = in_ch == out_ch\n",
    "        self.rs = res_scale\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BLOCK_3x3(in_ch, out_ch//squeeze, ker),\n",
    "            nn.BatchNorm2d(out_ch//squeeze),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BLOCK_3x3(out_ch//squeeze, out_ch, ker),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feat(x)\n",
    "        if self.skip: out = self.rs * out + x\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "Enocder:\n",
    "\"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.E = nn.Sequential(\n",
    "            HaarDWT(3),HaarDWT(12),\n",
    "            BLOCK_3x3(in_ch = 48, out_ch = 128, ker = 3, stride = 1),\n",
    "            RES_3x3_BLOCK1(in_ch = 128, out_ch = 128, ker = 3, squeeze = 4, res_scale = 1.0),\n",
    "            RES_3x3_BLOCK1(in_ch = 128, out_ch = 128, ker = 3, squeeze = 4, res_scale = 1.0),\n",
    "            nn.Conv2d(128, 3, 1),\n",
    "            QuantCLIP(8)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.E(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "Deocder:\n",
    "\"\"\"\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.D = nn.Sequential(\n",
    "            BLOCK_3x3(in_ch = 3, out_ch = 256, ker = 3, stride = 1),\n",
    "            RES_3x3_BLOCK1(in_ch = 256, out_ch = 256, ker = 3, squeeze = 4, res_scale = 1.0),\n",
    "            RES_3x3_BLOCK1(in_ch = 256, out_ch = 256, ker = 3, squeeze = 4, res_scale = 1.0),\n",
    "            RES_3x3_BLOCK1(in_ch = 256, out_ch = 256, ker = 3, squeeze = 4, res_scale = 1.0),\n",
    "            RES_3x3_BLOCK1(in_ch = 256, out_ch = 256, ker = 3, squeeze = 4, res_scale = 1.0),\n",
    "            nn.Conv2d(256, 48, 1),\n",
    "            HaarIDWT(12),HaarIDWT(3),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "\n",
    "        self.S = nn.Sequential(nn.ReflectionPad2d(1),\n",
    "                               nn.AvgPool2d(3, stride=1, padding=0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.D(x)\n",
    "#         x = self.S(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "de_model = Decoder()\n",
    "check_point_file = \"/home/cibitaw1/local/1WeStar/weights/submission_weights/decode.pth\"\n",
    "checkpoint = torch.load(check_point_file)\n",
    "de_model.load_state_dict(checkpoint, strict = False)\n",
    "de_model.cuda()\n",
    "print('.')\n",
    "\n",
    "en_model = Encoder()\n",
    "check_point_file = \"/home/cibitaw1/local/1WeStar/weights/submission_weights/encode.pth\"\n",
    "checkpoint = torch.load(check_point_file)\n",
    "en_model.load_state_dict(checkpoint, strict = False)\n",
    "en_model.cuda()\n",
    "print('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "de_model = Decoder()\n",
    "check_point_file = \"/media/cibitaw1/DATA/SP2020/compressACT/weights/\"+\\\n",
    "\"QuantACTShuffleV6_exp01/checkpoint.pth.tar\"\n",
    "checkpoint = torch.load(check_point_file)\n",
    "de_model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "de_model.cuda()\n",
    "print('.')\n",
    "\n",
    "en_model = Encoder()\n",
    "check_point_file = \"/media/cibitaw1/DATA/SP2020/compressACT/weights/\"+\\\n",
    "\"QuantACTShuffleV6_exp01/checkpoint.pth.tar\"\n",
    "checkpoint = torch.load(check_point_file)\n",
    "en_model.load_state_dict(checkpoint['state_dict'], strict = False)\n",
    "en_model.cuda()\n",
    "print('.')\n",
    "torch.save(de_model.state_dict(), \"/home/cibitaw1/local/1WeStar/weights/submission_weights/decode.pth\")\n",
    "torch.save(de_model.state_dict(), \"/home/cibitaw1/local/1WeStar/submission_package/decode.pth\")\n",
    "torch.save(en_model.state_dict(), \"/home/cibitaw1/local/1WeStar/weights/submission_weights/encode.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(I_org, model):\n",
    "\n",
    "    e_ = 512\n",
    "    c_ = 4\n",
    "    d_ = e_ // c_\n",
    "    pad_ = 4\n",
    "\n",
    "    w, h = I_org.size\n",
    "\n",
    "    comp_w_new = np.ceil(w/c_)\n",
    "    comp_h_new = np.ceil(h/c_)\n",
    "\n",
    "    new_w = int(e_ * np.ceil(w/e_))\n",
    "    new_h = int(e_ * np.ceil(h/e_))\n",
    "\n",
    "    com_w = new_w // c_\n",
    "    com_h = new_h // c_\n",
    "\n",
    "    I = np.uint8(I_org).copy()\n",
    "    I = np.pad(I, ((0, int(new_h - h)),\n",
    "                   (0, int(new_w - w)),\n",
    "                   (0, 0)), mode = \"reflect\")\n",
    "    I = Image.fromarray(I)\n",
    "\n",
    "\n",
    "    I1 = np.float32(I)/255.0\n",
    "    I1 = np.transpose(I1, [2, 0, 1])\n",
    "\n",
    "    Enout = np.zeros((3, com_h, com_w))\n",
    "    Enout_w = np.zeros((3, com_h, com_w))\n",
    "    for i in list(np.arange(0, new_h, e_)):\n",
    "        for j in list(np.arange(0, new_w, e_)):\n",
    "            if i == 0:\n",
    "                x1 = int(i)\n",
    "                x2 = int((i + e_) + (pad_*2*c_))\n",
    "            else:\n",
    "                x1 = int(i - (pad_*c_))\n",
    "                x2 = int((i + e_) + (pad_*c_))\n",
    "\n",
    "            if j == 0:\n",
    "                y1 = int(j)\n",
    "                y2 = int((j + e_) + (pad_*2*c_))\n",
    "            else:\n",
    "                y1 = int(j - (pad_*c_))\n",
    "                y2 = int((j + e_) + (pad_*c_))\n",
    "            It = torch.from_numpy(np.expand_dims(I1[:, x1:x2, y1:y2], 0))\n",
    "            Xe = model(It.cuda())\n",
    "            Xe = (Xe + 1.0)/2.0\n",
    "            Enout[:, x1//c_:x2//c_, y1//c_:y2//c_] += Xe.data.squeeze().cpu().numpy()\n",
    "            Enout_w[:, x1//c_:x2//c_, y1//c_:y2//c_] += 1.0\n",
    "\n",
    "    Enout = Enout/Enout_w\n",
    "    Enout = np.uint8(255 * Enout.transpose([1, 2, 0]))\n",
    "\n",
    "    Enout = Image.fromarray(Enout).crop((0, 0, comp_w_new, comp_h_new))\n",
    "\n",
    "    return Enout\n",
    "\n",
    "\n",
    "def decompress(EnIn, model, w, h):\n",
    "\n",
    "    e_ = 256\n",
    "    c_ = 4\n",
    "    d_ = e_ // c_\n",
    "    pad_ = 4\n",
    "\n",
    "#     w, h = int(EnIn.text['w']), int(EnIn.text['h'])\n",
    "\n",
    "    comp_w_new = np.ceil(w/c_)\n",
    "    comp_h_new = np.ceil(h/c_)\n",
    "\n",
    "    new_w = int(e_ * np.ceil(w/e_))\n",
    "    new_h = int(e_ * np.ceil(h/e_))\n",
    "\n",
    "    com_w = new_w // c_\n",
    "    com_h = new_h // c_\n",
    "\n",
    "\n",
    "    Iout = np.zeros((3,new_h,new_w), dtype = np.float32)\n",
    "    Iout_w = np.zeros((3,new_h,new_w), dtype = np.float32)\n",
    "\n",
    "    EnIn = np.uint8(EnIn).copy()\n",
    "    EnIn = np.pad(EnIn, ((0, int(com_h - EnIn.shape[0])),\n",
    "                         (0, int(com_w - EnIn.shape[1])),\n",
    "                         (0, 0)), mode = \"reflect\")\n",
    "\n",
    "\n",
    "    EnIn = np.float32(EnIn)/255.0\n",
    "    EnIn = np.transpose(EnIn, [2, 0, 1])\n",
    "    for i in list(np.arange(0, com_h, d_)):\n",
    "        for j in list(np.arange(0, com_w, d_)):\n",
    "\n",
    "            if i == 0:\n",
    "                x1 = int(i)\n",
    "                x2 = int((i + d_) + pad_*2)\n",
    "            else:\n",
    "                x1 = int(i - pad_)\n",
    "                x2 = int((i + d_) + pad_)\n",
    "\n",
    "            if j == 0:\n",
    "                y1 = int(j)\n",
    "                y2 = int((j + d_) + pad_*2)\n",
    "            else:\n",
    "                y1 = int(j - pad_)\n",
    "                y2 = int((j + d_) + pad_)\n",
    "\n",
    "            It = torch.from_numpy(np.expand_dims(EnIn[:, x1:x2, y1:y2], 0))\n",
    "            It = It * 2.0 - 1.0\n",
    "            Xe = model(It.cuda()).data.squeeze().cpu()\n",
    "\n",
    "            Iout[:, x1*c_:x2*c_, y1*c_:y2*c_] += np.clip(Xe.numpy(), 0, 1)\n",
    "            Iout_w[:, x1*c_:x2*c_, y1*c_:y2*c_] += 1.0\n",
    "\n",
    "    Iout = Iout/Iout_w\n",
    "\n",
    "    Iout = np.uint8(255 * Iout.transpose([1, 2, 0]))\n",
    "    Iout = Image.fromarray(Iout).crop((0, 0, w, h))\n",
    "\n",
    "    return Iout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = \"/media/cibitaw1/DATA/super_rez/professional_valid/valid/alberto-montalesi-176097.png\"\n",
    "I = Image.open(img_file).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enout = compress(I, en_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpg_enc(Enout, \"test_en.bpg\", \"bpgenc\", I.size[0], I.size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enout, w, h = bpg_dec(\"test_en.bpg\", \"bpgdec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iout = decompress(Enout, de_model, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_fldr = \"/media/cibitaw1/DATA/super_rez/professional_valid/valid\"\n",
    "imgs = glob(src_fldr + os.sep + \"*.png\")\n",
    "src_fldr = \"/media/cibitaw1/DATA/super_rez/mobile_valid/valid\"\n",
    "imgs += glob(src_fldr + os.sep + \"*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_fldr = \"/media/cibitaw1/DATA/super_rez/comp_test/compressed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [01:05<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for img in tqdm(imgs):\n",
    "    I = Image.open(img).convert(\"RGB\")\n",
    "    Enout = compress(I, en_model)\n",
    "    img_name = os.path.join(dst_fldr, img.split(os.sep)[-1]).replace(\".png\", \".bpg\")\n",
    "    bpg_enc(Enout, img_name, \"bpgenc\", I.size[0], I.size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:07<00:00, 13.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for img in tqdm(imgs):\n",
    "    I = Image.open(img).convert(\"RGB\")\n",
    "    img_name = os.path.join(dst_fldr, img.split(os.sep)[-1]).replace(\".png\", \".bpg\")\n",
    "    new_img_name = img_name + '__w_' + str(I.size[0]) + '__h_' + str(I.size[1])\n",
    "    os.rename(img_name, new_img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpg_dec(bpg_enc_file, dec_cmd):\n",
    "#     bpg_enc_file = bpg_enc_file.replace(\" \", \"\\ \")\n",
    "    x = bpg_enc_file.split('__')\n",
    "    bpg_enc_file = x[0]\n",
    "    w = int(x[1].replace(\"w_\", \"\"))\n",
    "    h = int(x[2].replace(\"h_\", \"\"))\n",
    "    os.system(dec_cmd + ' \"' + bpg_enc_file + '\" -o test_de.png')\n",
    "    # h = int(os.getxattr(bpg_enc_file, 'user.h'))\n",
    "    # w = int(os.getxattr(bpg_enc_file, 'user.w'))\n",
    "    I = Image.open(\"test_de.png\")\n",
    "    os.remove(\"test_de.png\")\n",
    "    return I, w, h, bpg_enc_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bpg_dec(\"/media/cibitaw1/DATA/super_rez/comp_test/images/IMG_20170725_123034.bpg__w_2024__h_1518\", \"bpgdec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x341 at 0x7F7180389AC8>,\n",
       " 2024,\n",
       " 1518,\n",
       " '/media/cibitaw1/DATA/super_rez/comp_test/images/IMG_20170725_123034.bpg')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/cibitaw1/DATA/super_rez/comp_test/images/IMG_20170725_123034.bpg',\n",
       " 'w_2024',\n",
       " 'h_1518']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.split('__')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
